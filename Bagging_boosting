#Bagging and Boosting

import pandas as pd
from sklearn import tree
from sklearn.model_selection import train_test_split
import graphviz
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier

faults = ["Pastry","Z_Scratch","K_Scatch","Stains","Dirtiness","Bumps","Other_Faults"]

def read_in_dataset(faults):
    df = pd.read_csv("faults.csv")
    df["fault"] = 0
    #Create the categorical variable for each fault
    for i in range(0, len(faults)):
        #Indexes of faults
        true_fault_indexes = df.loc[df[faults[i]] == 1].index.tolist()
        df.loc[true_fault_indexes, "fault"] = i+1
    return df

df = read_in_dataset(faults)
#print(df.head())
    
#Create the training and test set
drop_features = ["fault"] + faults
features = df.drop(drop_features, axis=1)
outcomes = df["fault"]

training_features, test_features, training_outcomes, test_outcomes = train_test_split(features, outcomes, test_size=0.1, random_state=4)
#model = tree.DecisionTreeClassifier(max_depth=5)
#model = RandomForestClassifier(max_features=7) 
#model = BaggingClassifier(max_samples=0.1, n_estimators=10)

#Adaboost adjust weights of predictions for random resampling
#model = AdaBoostClassifier(n_estimators=100, learning_rate=0.1)

#Gradientboost 
#model = GradientBoostingClassifier(n_estimators=50)

#Rule of thumb is that if you have around 100% on training accuracy is overfitting

model.fit(training_features, training_outcomes)
test_accuracy_score = model.score(test_features, test_outcomes)
training_accuracy_score = model.score(training_features, training_outcomes)
print(f"Training accuracy {training_accuracy_score}")
print(f"Test accuracy {test_accuracy_score}")



